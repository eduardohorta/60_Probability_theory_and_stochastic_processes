\documentclass[12pt]{article}
\usepackage{pmmeta}
\pmcanonicalname{ProbabilityDistributionFunction}
\pmcreated{2013-03-22 12:37:25}
\pmmodified{2013-03-22 12:37:25}
\pmowner{Mathprof}{13753}
\pmmodifier{Mathprof}{13753}
\pmtitle{probability distribution function}
\pmrecord{11}{32884}
\pmprivacy{1}
\pmauthor{Mathprof}{13753}
\pmtype{Definition}
\pmcomment{trigger rebuild}
\pmclassification{msc}{60E99}
\pmsynonym{probability density function}{ProbabilityDistributionFunction}
\pmsynonym{distribution}{ProbabilityDistributionFunction}
%\pmkeywords{distribution}
%\pmkeywords{random variable}
\pmrelated{Measure}
\pmrelated{Stochastic}
\pmrelated{DiscreteDensityFunction}
\pmrelated{DistributionFunction}
\pmrelated{DensityFunction}
\pmrelated{AreaUnderGaussianCurve}
\pmdefines{cumulative distribution function}

\endmetadata

\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsfonts}

% used for TeXing text within eps files
%\usepackage{psfrag}
% need this for including graphics (\includegraphics)
%\usepackage{graphicx}
% for neatly defining theorems and propositions
%\usepackage{amsthm}
% making logically defined graphics
%%%\usepackage{xypic} 

% there are many more packages, add them here as you need them

% define commands here
\newcommand{\md}{d}
\newcommand{\mv}[1]{\mathbf{#1}}	% matrix or vector
\newcommand{\mvt}[1]{\mv{#1}^{\mathrm{T}}}
\newcommand{\mvi}[1]{\mv{#1}^{-1}}
\newcommand{\mderiv}[1]{\frac{\md}{\md {#1}}} %d/dx
\newcommand{\mnthderiv}[2]{\frac{\md^{#2}}{\md {#1}^{#2}}} %d^n/dx
\newcommand{\mpderiv}[1]{\frac{\partial}{\partial {#1}}} %partial d^n/dx
\newcommand{\mnthpderiv}[2]{\frac{\partial^{#2}}{\partial {#1}^{#2}}} %partial d^n/dx
\newcommand{\borel}{\mathfrak{B}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\rationals}{\mathbb{Q}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complexes}{\mathbb{C}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\defined}{:=}
\newcommand{\var}{\mathrm{var}}
\newcommand{\cov}{\mathrm{cov}}
\newcommand{\corr}{\mathrm{corr}}
\newcommand{\set}[1]{\left\{#1\right\}}
\newcommand{\powerset}[1]{\mathcal{P}(#1)}
\newcommand{\bra}[1]{\langle#1 \vert}
\newcommand{\ket}[1]{\vert \hspace{1pt}#1\rangle}
\newcommand{\braket}[2]{\langle #1 \ket{#2}}
\newcommand{\abs}[1]{\left|#1\right|}
\newcommand{\norm}[1]{\left|\left|#1\right|\right|}
\newcommand{\esssup}{\mathrm{ess\ sup}}
\newcommand{\Lspace}[1]{L^{#1}}
\newcommand{\Lone}{\Lspace{1}}
\newcommand{\Ltwo}{\Lspace{2}}
\newcommand{\Lp}{\Lspace{p}}
\newcommand{\Lq}{\Lspace{q}}
\newcommand{\Linf}{\Lspace{\infty}}
\newcommand{\sequence}[1]{\{#1\}}
\begin{document}
\section{Definition}

Let $(\Omega, \borel, \mu)$ be a measure space.  A {\em probability distribution function} on $\Omega$ is a function $f: \Omega \longrightarrow \reals$ such that:
\begin{enumerate}
\item $f$ is $\mu$-measurable
\item $f$ is nonnegative $\mu$-almost everywhere.
\item $f$ satisfies the equation
$$
\int_{\Omega} f(x)\ d\mu = 1
$$
\end{enumerate}

The main feature of a probability distribution function is that it induces a probability measure $P$ on the measure space $(\Omega, \borel)$, given by
$$
P(A) := \int_A f(x)\ d\mu = \int_{\Omega} 1_A f(x)\ d\mu,
$$
for all $A \in \borel$. The measure $P$ is called the {\em associated probability measure} of $f$. Note that $P$ and $\mu$ are different measures, \PMlinkescapetext{even} though they both share the same underlying measurable space $(\Omega, \borel)$.
\section{Examples}

\subsection{Discrete case} Let $I$ be a countable set, and impose the counting measure on $I$ ($\mu(A) := |A|$, the cardinality of $A$, for any subset $A \subset I$). A probability distribution function on $I$ is then a nonnegative function $f: I \longrightarrow \reals$ satisfying the equation
$$
\sum_{i \in I} f(i) = 1.
$$

One example is the Poisson distribution $P_r$ on $\naturals$ (for any real number $r$), which is given by 
$$
P_r(i) := e^{-r} \frac{r^i}{i!}
$$
for any $i \in \naturals$.

Given any probability space $(\Omega, \borel, \mu)$ and any random variable $X: \Omega \longrightarrow I$, we can form a distribution function on $I$ by taking $f(i) := \mu(\{X = i\})$. The resulting function is called the distribution of $X$ on $I$.

\subsection{Continuous case}
Suppose $(\Omega, \borel, \mu)$ equals $(\reals, \borel_\lambda, \lambda)$, the real numbers equipped with Lebesgue measure. Then a probability distribution function $f: \reals \longrightarrow \reals$ is simply a measurable, nonnegative almost everywhere function such that
$$
\int_{-\infty}^\infty f(x)\ dx = 1.
$$
The associated measure has \PMlinkname{Radon--Nikodym derivative}{RadonNikodymTheorem} with respect to $\lambda$ equal to $f$:
$$
\frac{dP}{d\lambda} = f.
$$
One defines the {\em cumulative distribution function} $F$ of $f$ by the formula
$$
F(x) := P(\{X \leq x\}) = \int_{-\infty}^x f(t)\ dt,
$$
for all $x \in \reals$. A well known example of a probability distribution function on $\reals$ is the Gaussian distribution, or normal distribution
$$
f(x) := \frac{1}{\sigma \sqrt{2 \pi}} e^{-(x-m)^2/2\sigma^2}.
$$
%%%%%
%%%%%
\end{document}
